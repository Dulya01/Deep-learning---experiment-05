# -*- coding: utf-8 -*-
"""deepL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iLS3Y7c0X10ApzBGfqobM69wFP0npyUs
"""

pip install nltk

from google.colab import files
uploaded = files.upload()

import torch
import torch.nn as nn
import torch.optim as optim
import random
import re
import numpy as np
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from sklearn.model_selection import train_test_split
from collections import Counter
from torch.utils.data import Dataset, DataLoader

# Set random seeds for reproducibility
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

# ---------------------
# 1. Preprocessing
# ---------------------
def load_data(filepath, num_samples=10000):
    with open(filepath, 'r', encoding='utf-8') as f:
        lines = f.read().strip().split('\n')
    sentence_pairs = [line.split('\t') for line in lines[:num_samples]]
    return zip(*sentence_pairs)

def tokenize(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z¿¡?.]+", r" ", text)
    return text.strip().split()

class Vocab:
    def __init__(self, sentences, min_freq=1):
        self.word2idx = {"<pad>": 0, "<sos>": 1, "<eos>": 2, "<unk>": 3}
        self.idx2word = {0: "<pad>", 1: "<sos>", 2: "<eos>", 3: "<unk>"}
        self.freqs = Counter()
        self.build_vocab(sentences, min_freq)

    def build_vocab(self, sentences, min_freq):
        for sentence in sentences:
            self.freqs.update(sentence)

        for word, freq in self.freqs.items():
            if freq >= min_freq and word not in self.word2idx:
                idx = len(self.word2idx)
                self.word2idx[word] = idx
                self.idx2word[idx] = word

    def encode(self, sentence):
        return [self.word2idx.get(word, self.word2idx["<unk>"]) for word in sentence]

    def decode(self, indices):
        return [self.idx2word.get(idx, "<unk>") for idx in indices]

    def __len__(self):
        return len(self.word2idx)

# ---------------------
# 2. Dataset
# ---------------------
class TranslationDataset(Dataset):
    def __init__(self, src_sentences, trg_sentences, src_vocab, trg_vocab, max_len=20):
        self.src_sentences = src_sentences
        self.trg_sentences = trg_sentences
        self.src_vocab = src_vocab
        self.trg_vocab = trg_vocab
        self.max_len = max_len

    def __len__(self):
        return len(self.src_sentences)

    def __getitem__(self, idx):
        src = self.src_vocab.encode(self.src_sentences[idx])
        trg = self.trg_vocab.encode(self.trg_sentences[idx])
        src = src[:self.max_len]
        trg = [self.trg_vocab.word2idx["<sos>"]] + trg[:self.max_len - 2] + [self.trg_vocab.word2idx["<eos>"]]

        src += [0] * (self.max_len - len(src))
        trg += [0] * (self.max_len - len(trg))

        return torch.tensor(src, dtype=torch.long), torch.tensor(trg, dtype=torch.long)

# ---------------------
# 3. Model
# ---------------------
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True)

    def forward(self, src):
        embedded = self.embedding(src)
        outputs, (hidden, cell) = self.lstm(embedded)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim, output_dim)

    def forward(self, input, hidden, cell):
        input = input.unsqueeze(1)
        embedded = self.embedding(input)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(1))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size, trg_len = trg.shape
        trg_vocab_size = self.decoder.embedding.num_embeddings
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)

        hidden, cell = self.encoder(src)
        input = trg[:, 0]

        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t] = output
            top1 = output.argmax(1)
            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1

        return outputs

# ---------------------
# 4. Training
# ---------------------
def train_model(model, iterator, optimizer, criterion, device):
    model.train()
    epoch_loss = 0

    for src, trg in iterator:
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad()
        output = model(src, trg)
        output_dim = output.shape[-1]
        output = output[:, 1:].reshape(-1, output_dim)
        trg = trg[:, 1:].reshape(-1)

        loss = criterion(output, trg)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    return epoch_loss / len(iterator)

# ---------------------
# 5. Inference & BLEU
# ---------------------
def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=20):
    model.eval()
    tokens = tokenize(sentence)
    tokens = tokens[:max_len]
    src_encoded = src_vocab.encode(tokens)
    src_encoded += [0] * (max_len - len(src_encoded))
    src_tensor = torch.tensor([src_encoded], device=device)

    hidden, cell = model.encoder(src_tensor)
    input = torch.tensor([trg_vocab.word2idx["<sos>"]], device=device)

    outputs = []
    for _ in range(max_len):
        output, hidden, cell = model.decoder(input, hidden, cell)
        top1 = output.argmax(1).item()
        if top1 == trg_vocab.word2idx["<eos>"]:
            break
        outputs.append(top1)
        input = torch.tensor([top1], device=device)

    return trg_vocab.decode(outputs)

# ---------------------
# 6. Run All
# ---------------------
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    eng_sentences, spa_sentences = load_data('spa.txt')
    eng_tokens = [tokenize(sent) for sent in eng_sentences]
    spa_tokens = [tokenize(sent) for sent in spa_sentences]

    src_vocab = Vocab(eng_tokens)
    trg_vocab = Vocab(spa_tokens)

    train_src, val_src, train_trg, val_trg = train_test_split(eng_tokens, spa_tokens, test_size=0.2)
    train_dataset = TranslationDataset(train_src, train_trg, src_vocab, trg_vocab)
    val_dataset = TranslationDataset(val_src, val_trg, src_vocab, trg_vocab)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    enc = Encoder(len(src_vocab), 256, 512, 2)
    dec = Decoder(len(trg_vocab), 256, 512, 2)
    model = Seq2Seq(enc, dec, device).to(device)

    optimizer = optim.Adam(model.parameters())
    criterion = nn.CrossEntropyLoss(ignore_index=0)

    losses = []

    for epoch in range(10):
        loss = train_model(model, train_loader, optimizer, criterion, device)
        losses.append(loss)
        print(f"Epoch {epoch+1}: Loss = {loss:.4f}")

    # Plotting the training loss
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, len(losses) + 1), losses, marker='o', color='b')
    plt.title('Training Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Example translation
    test_sentence = "how are you"
    print("Translation:", " ".join(translate_sentence(model, test_sentence, src_vocab, trg_vocab, device)))