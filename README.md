English-to-Spanish Translation using Seq2Seq LSTM Models

This project implements a Sequence-to-Sequence (Seq2Seq) model for English-to-Spanish translation using Long Short-Term Memory (LSTM) networks. The objective is to explore and compare different Seq2Seq architectures, with and without attention mechanisms.

Experiment Overview :

Two major architectures are explored;

LSTM Encoder-Decoder (without Attention)
A baseline Seq2Seq model using plain LSTM layers for encoding and decoding.

LSTM Encoder-Decoder (with Attention)
Enhanced translation models using attention mechanisms:

      Bahdanau Attention (Additive)
      Luong Attention (Multiplicative)
